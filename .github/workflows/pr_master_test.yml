name: GitHub CI

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
  workflow_dispatch:
    inputs:
      git-ref:
        description: Git Hash (Optional)
        required: false

defaults:
  run:
    shell: bash -l {0}

env:
  PYTHONWARNINGS: ignore::UserWarning
  PYTHON_CORE_PKGS: wheel
  PYTHON_REQUIRED_PKGS: >
      nose coverage
  PYTHON_BASE_PKGS: >
      ply six dill ipython networkx openpyxl pathos
      pint pyro4 pyyaml sphinx_rtd_theme sympy xlrd
      python-louvain
  PYTHON_NUMPY_PKGS: >
      numpy scipy pyodbc pandas matplotlib seaborn numdifftools
  CACHE_VER: v3
  NEOS_EMAIL: tests@pyomo.org

jobs:
  build:
    name: ${{ matrix.TARGET }}/${{ matrix.python }}${{ matrix.other }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-18.04, macos-latest, windows-latest]
        python: [3.6, 3.7, 3.8, 3.9, pypy3]
        other: [""]
        category: ["nightly"]
        # Ubuntu-18.04 should be replaced with ubuntu-latest once PyNumero
        # build error is resolved:
        # https://github.com/Pyomo/pyomo/issues/1710

        include:
        - os: ubuntu-18.04
          TARGET: linux
          PYENV: pip

        - os: macos-latest
          TARGET: osx
          PYENV: pip

        - os: windows-latest
          TARGET: win
          PYENV: conda
          PACKAGES: glpk

        - os: ubuntu-18.04
          python: 3.8
          other: /mpi
          mpi: 3
          skip_doctest: 1
          TARGET: linux
          PYENV: conda
          PACKAGES: mpi4py openmpi

        - os: ubuntu-18.04
          python: 3.7
          other: /conda
          skip_doctest: 1
          TARGET: linux
          PYENV: conda
          PACKAGES: mpi4py openmpi

        - os: ubuntu-18.04
          python: 3.9
          other: /slim
          slim: 1
          skip_doctest: 1
          TARGET: linux
          PYENV: pip

        - os: ubuntu-18.04
          python: 3.6
          other: /cython
          setup_options: --with-cython
          skip_doctest: 1
          TARGET: linux
          PYENV: pip
          PACKAGES: cython

        - os: ubuntu-18.04
          python: 3.7
          other: /singletest
          category: neos
          skip_doctest: 1
          TARGET: linux
          PYENV: pip

        exclude:
        - {os: macos-latest, python: pypy3}
        - {os: windows-latest, python: pypy3}

    steps:
    - name: Checkout Pyomo source
      uses: actions/checkout@v2

    - name: Configure job parameters
      run: |
        JOB="${{matrix.TARGET}}/${{matrix.python}}${{matrix.other}}"
        echo "GHA_JOBNAME=$JOB" | sed 's|/|_|g' >> $GITHUB_ENV
        if test -z "${{matrix.other}}"; then
            echo "GHA_JOBGROUP=${{matrix.TARGET}}" >> $GITHUB_ENV
        else
            echo "GHA_JOBGROUP=other" >> $GITHUB_ENV
        fi
        # Note: pandas 1.0.3 causes gams 29.1.0 import to fail in python 3.8
        PYTHON_PACKAGES="${PYTHON_REQUIRED_PKGS}"
        if test -z "${{matrix.slim}}"; then
            PYTHON_PACKAGES="$PYTHON_PACKAGES ${PYTHON_BASE_PKGS}"
        fi
        if [[ ${{matrix.python}} != pypy* && ! "${{matrix.slim}}" ]]; then
            # NumPy and derivatives either don't build under pypy, or if
            # they do, the builds take forever.
            PYTHON_PACKAGES="$PYTHON_PACKAGES ${PYTHON_NUMPY_PKGS}"
        fi
        PYTHON_PACKAGES="$PYTHON_PACKAGES ${{matrix.PACKAGES}}"
        echo "PYTHON_PACKAGES=$PYTHON_PACKAGES" \
            | tr '\n' ' ' | sed 's/ \+/ /g' >> $GITHUB_ENV

    # Ideally we would cache the conda downloads; however, each cache is
    # over 850MB, and with 5 python versions, that would consume 4.2 of
    # the 5 GB GitHub allows.
    #
    #- name: Conda package cache
    #  uses: actions/cache@v2
    #  if: matrix.PYENV == 'conda'
    #  id: conda-cache
    #  with:
    #    path: cache/conda
    #    key: conda-${{env.CACHE_VER}}.0-${{runner.os}}-${{matrix.python}}

    - name: Pip package cache
      uses: actions/cache@v2
      if: matrix.PYENV == 'pip'
      id: pip-cache
      with:
        path: cache/pip
        key: pip-${{env.CACHE_VER}}.0-${{runner.os}}-${{matrix.python}}

    - name: OS package cache
      uses: actions/cache@v2
      if: matrix.TARGET != 'osx'
      id: os-cache
      with:
        path: cache/os
        key: pkg-${{env.CACHE_VER}}.0-${{runner.os}}

    - name: TPL package download cache
      uses: actions/cache@v2
      id: download-cache
      with:
        path: cache/download
        key: download-${{env.CACHE_VER}}.0-${{runner.os}}

    - name: Configure curl
      run: |
        CURLRC="$(cat <<EOF
           retry = 0
           max-time = 30
        EOF
        )"
        echo "$CURLRC" > ${GITHUB_WORKSPACE}/.curlrc
        echo "$CURLRC" > ${GITHUB_WORKSPACE}/_curlrc
        echo "CURL_HOME=$GITHUB_WORKSPACE" >> $GITHUB_ENV

    - name: Update OSX
      if: matrix.TARGET == 'osx'
      run: |
        mkdir -p ${GITHUB_WORKSPACE}/cache/os
        export HOMEBREW_CACHE=${GITHUB_WORKSPACE}/cache/os
        # Be cautious running brew update: it can break
        #    setup-python on OSX
        # brew update
        #
        # Notes:
        #  - install glpk
        #  - pyodbc needs: gcc pkg-config unixodbc freetds
        for pkg in bash pkg-config unixodbc freetds glpk; do
            brew list $pkg || brew install $pkg
        done

    - name: Update Linux
      if: matrix.TARGET == 'linux'
      run: |
        mkdir -p ${GITHUB_WORKSPACE}/cache/os
        # Notes:
        #  - install glpk
        #  - ipopt needs: libopenblas-dev gfortran liblapack-dev
        sudo apt-get -o Dir::Cache=${GITHUB_WORKSPACE}/cache/os \
            install libopenblas-dev gfortran liblapack-dev glpk-utils
        sudo chmod -R 777 ${GITHUB_WORKSPACE}/cache/os

    - name: Set up Python ${{ matrix.python }}
      if: matrix.PYENV == 'pip'
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python }}

    - name: Set up Miniconda Python ${{ matrix.python }}
      if: matrix.PYENV == 'conda'
      uses: conda-incubator/setup-miniconda@v2
      with:
        auto-update-conda: true
        python-version: ${{ matrix.python }}

    # GitHub actions is very fragile when it comes to setting up various
    # Python interpreters, expecially the setup-miniconda interface.
    # Per the setup-miniconda documentation, it is important to always
    # invoke bash as a login shell ('shell: bash -l {0}') so that the
    # conda environment is properly activated.  However, running within
    # a login shell appears to foul up the link to python from
    # setup-python.  Further, we have anecdotal evidence that
    # subprocesses invoked through $(python -c ...) and `python -c ...`
    # will not pick up the python activated by setup-python on OSX.
    #
    # Our solution is to define a PYTHON_EXE environment variable that
    # can be explicitly called within subprocess calls to reach the
    # correct interpreter.  Note that we must explicitly run in a *non*
    # login shell to set up the environment variable for the
    # setup-python environments.

    - name: Install Python Packages (pip)
      if: matrix.PYENV == 'pip'
      shell: bash # DO NOT REMOVE: see note above
      run: |
        python -c 'import sys;print(sys.executable)'
        python -m pip install --cache-dir cache/pip --upgrade pip
        pip install --cache-dir cache/pip ${PYTHON_CORE_PKGS}
        pip install --cache-dir cache/pip ${PYTHON_PACKAGES}
        # pymysql 1.0.0: No longer supports Python 2 or pypy2
        pip install --cache-dir cache/pip pymysql || \
            pip install --cache-dir cache/pip pymysql==0.10.1
        if test -z "${{matrix.slim}}"; then
            pip install --cache-dir cache/pip cplex \
                || echo "WARNING: CPLEX Community Edition is not available"
            pip install --cache-dir cache/pip \
                -i https://pypi.gurobi.com gurobipy \
                || echo "WARNING: Gurobi is not available"
            pip install --cache-dir cache/pip xpress \
                || echo "WARNING: Xpress Community Edition is not available"
            pip install --cache-dir cache/pip z3-solver \
                || echo "WARNING: Z3 Solver is not available"
        fi
        python -c 'import sys; print("PYTHON_EXE=%s" \
            % (sys.executable,))' >> $GITHUB_ENV
        echo "NOSETESTS="`which nosetests` >> $GITHUB_ENV

    - name: Install Python packages (conda)
      if: matrix.PYENV == 'conda'
      run: |
        mkdir -p $GITHUB_WORKSPACE/cache/conda
        conda config --set always_yes yes
        conda config --set auto_update_conda false
        conda config --prepend pkgs_dirs $GITHUB_WORKSPACE/cache/conda
        conda info
        conda config --show-sources
        conda list --show-channel-urls
        conda install -q -y -c conda-forge ${PYTHON_CORE_PKGS}
        conda install -q -y -c conda-forge ${PYTHON_PACKAGES}
        if test -z "${{matrix.slim}}"; then
            conda install -q -y -c ibmdecisionoptimization 'cplex>=12.10' \
                || echo "WARNING: CPLEX Community Edition is not available"
            conda install -q -y -c fico-xpress xpress \
                || echo "WARNING: Xpress Community Edition is not available"
            for PKG in casadi cyipopt pymumps; do
                conda install -q -y -c conda-forge $PKG \
                    || echo "WARNING: $PKG is not available"
            done
        fi
        python -c 'import sys; print("PYTHON_EXE=%s" \
            % (sys.executable,))' >> $GITHUB_ENV
        echo "NOSETESTS="`which nosetests` >> $GITHUB_ENV

    - name: Setup TPL package directories
      run: |
        TPL_DIR="${GITHUB_WORKSPACE}/cache/tpl"
        mkdir -p "$TPL_DIR"
        DOWNLOAD_DIR="${GITHUB_WORKSPACE}/cache/download"
        mkdir -p "$DOWNLOAD_DIR"
        echo "TPL_DIR=$TPL_DIR" >> $GITHUB_ENV
        echo "DOWNLOAD_DIR=$DOWNLOAD_DIR" >> $GITHUB_ENV

    - name: Install Ipopt
      if: ${{ ! matrix.slim }}
      run: |
        IPOPT_DIR=$TPL_DIR/ipopt
        echo "$IPOPT_DIR" >> $GITHUB_PATH
        mkdir -p $IPOPT_DIR
        IPOPT_TAR=${DOWNLOAD_DIR}/ipopt.tar.gz
        if test ! -e $IPOPT_TAR; then
            echo "...downloading Ipopt"
            if test "${{matrix.TARGET}}" == osx; then
                echo "IDAES Ipopt not available on OSX"
                exit 0
            fi
            URL=https://github.com/IDAES/idaes-ext
            RELEASE=$(curl --max-time 150 --retry 8 \
                -L -s -H 'Accept: application/json' ${URL}/releases/latest)
            VER=$(echo $RELEASE | sed -e 's/.*"tag_name":"\([^"]*\)".*/\1/')
            URL=${URL}/releases/download/$VER
            if test "${{matrix.TARGET}}" == linux; then
                curl --max-time 150 --retry 8 \
                    -L $URL/idaes-solvers-ubuntu1804-64.tar.gz \
                    > $IPOPT_TAR
            else
                curl --max-time 150 --retry 8 \
                    -L $URL/idaes-solvers-windows-64.tar.gz \
                    $URL/idaes-lib-windows-64.tar.gz > $IPOPT_TAR
            fi
        fi
        cd $IPOPT_DIR
        tar -xzi < $IPOPT_TAR
        echo ""
        echo "$IPOPT_DIR"
        ls -l $IPOPT_DIR

    - name: Install GAMS
      if: ${{ ! matrix.slim }}
      # We install using Powershell because the GAMS installer hangs
      # when launched from bash on Windows
      shell: pwsh
      run: |
        $GAMS_DIR = "${env:TPL_DIR}/gams"
        echo "$GAMS_DIR" | `
            Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
        echo "LD_LIBRARY_PATH=${env:LD_LIBRARY_PATH}:$GAMS_DIR" `
            Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
        echo "DYLD_LIBRARY_PATH=${env:DYLD_LIBRARY_PATH}:$GAMS_DIR" `
            Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
        $INSTALLER = "${env:DOWNLOAD_DIR}/gams_install.exe"
        $URL = "https://d37drm4t2jghv5.cloudfront.net/distributions/29.1.0"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            $URL = "$URL/windows/windows_x64_64.exe"
        } elseif ( "${{matrix.TARGET}}" -eq "osx" ) {
            $URL = "$URL/macosx/osx_x64_64_sfx.exe"
        } else {
            $URL = "$URL/linux/linux_x64_64_sfx.exe"
        }
        if (-not (Test-Path "$INSTALLER" -PathType Leaf)) {
            echo "...downloading GAMS"
            Invoke-WebRequest -Uri "$URL" -OutFile "$INSTALLER"
        }
        echo "...installing GAMS"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            Start-Process -FilePath "$INSTALLER" -ArgumentList `
                "/SP- /NORESTART /VERYSILENT /DIR=$GAMS_DIR /NOICONS" `
                -Wait
        } else {
            chmod 777 $INSTALLER
            Start-Process -FilePath "$INSTALLER" -ArgumentList `
                "-q -d $GAMS_DIR" -Wait
            mv $GAMS_DIR/*/* $GAMS_DIR/.
        }
        echo ""
        echo "$GAMS_DIR"
        ls -l $GAMS_DIR

    - name: Install GAMS Python bindings
      if: ${{ ! matrix.slim }}
      run: |
        GAMS_DIR="${env:TPL_DIR}/gams"
        py_ver=$($PYTHON_EXE -c 'import sys;v="_%s%s" % sys.version_info[:2] \
            ;print(v if v != "_27" else "")')
        if test -e $GAMS_DIR/apifiles/Python/api$py_ver; then
            echo "Installing GAMS Python bindings"
            pushd $GAMS_DIR/apifiles/Python/api$py_ver
            $PYTHON_EXE setup.py install
            popd
        fi

    - name: Install BARON
      if: ${{ ! matrix.slim }}
      shell: pwsh
      run: |
        $BARON_DIR = "${env:TPL_DIR}/baron"
        echo "$BARON_DIR" | `
            Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
        $URL = "https://www.minlp.com/downloads/xecs/baron/current/"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            $INSTALLER = "${env:DOWNLOAD_DIR}/baron_install.exe"
            $URL += "baron-win64.exe"
        } elseif ( "${{matrix.TARGET}}" -eq "osx" ) {
            $INSTALLER = "${env:DOWNLOAD_DIR}/baron_install.zip"
            $URL += "baron-osx64.zip"
        } else {
            $INSTALLER = "${env:DOWNLOAD_DIR}/baron_install.zip"
            $URL += "baron-lin64.zip"
        }
        if (-not (Test-Path "$INSTALLER" -PathType Leaf)) {
            echo "...downloading BARON ($URL)"
            Invoke-WebRequest -Uri "$URL" -OutFile "$INSTALLER"
        }
        echo "...installing BARON"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            Start-Process -FilePath "$INSTALLER" -ArgumentList `
                "/SP- /NORESTART /VERYSILENT /DIR=$BARON_DIR /NOICONS" `
                -Wait
        } else {
            unzip -q $INSTALLER
            mv baron-* $BARON_DIR
        }
        echo ""
        echo "$BARON_DIR"
        ls -l $BARON_DIR

    - name: Install GJH_ASL_JSON
      if: ${{ ! matrix.slim && matrix.TARGET != 'win' }}
      run: |
        GJH_DIR="$TPL_DIR/gjh"
        echo "${GJH_DIR}" >> $GITHUB_PATH
        INSTALL_DIR="${DOWNLOAD_DIR}/gjh"
        if test ! -e "$INSTALL_DIR/bin"; then
            mkdir -p "$INSTALL_DIR"
            INSTALLER="$INSTALL_DIR/gjh_asl_json.zip"
            URL="https://codeload.github.com/ghackebeil/gjh_asl_json/zip/master"
            curl --max-time 150 --retry 8 -L $URL > $INSTALLER
            cd $INSTALL_DIR
            unzip -q $INSTALLER
            cd gjh_asl_json-master/Thirdparty
            ./get.ASL
            cd ..
            make
            mv bin "$INSTALL_DIR/bin"
        fi
        cp -rp "$INSTALL_DIR/bin" "$GJH_DIR"
        echo ""
        echo "$GJH_DIR"
        ls -l $GJH_DIR

    - name: Install Pyomo and PyUtilib
      run: |
        echo ""
        echo "Clone Pyomo-model-libraries..."
        git clone https://github.com/Pyomo/pyomo-model-libraries.git
        echo ""
        echo "Install PyUtilib..."
        echo ""
        $PYTHON_EXE -m pip install git+https://github.com/PyUtilib/pyutilib
        echo ""
        echo "Install Pyomo..."
        echo ""
        $PYTHON_EXE setup.py develop ${{matrix.setup_options}}
        echo ""
        echo "Set custom PYOMO_CONFIG_DIR"
        echo ""
        echo "PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config" >> $GITHUB_ENV

    - name: Set up coverage tracking
      run: |
        if test "${{matrix.TARGET}}" == win; then
            COVERAGE_BASE=${GITHUB_WORKSPACE}\\.cover
        else
            COVERAGE_BASE=${GITHUB_WORKSPACE}/.cover
        fi
        COVERAGE_RC=${COVERAGE_BASE}_rc
        echo "COVERAGE_RCFILE=$COVERAGE_RC" >> $GITHUB_ENV
        echo "COVERAGE_PROCESS_START=$COVERAGE_RC" >> $GITHUB_ENV
        cp ${GITHUB_WORKSPACE}/.coveragerc ${COVERAGE_RC}
        echo "data_file=${COVERAGE_BASE}age" >> ${COVERAGE_RC}
        SITE_PACKAGES=$($PYTHON_EXE -c "from distutils.sysconfig import \
            get_python_lib; print(get_python_lib())")
        echo "Python site-packages: $SITE_PACKAGES"
        echo 'import coverage; coverage.process_startup()' \
            > ${SITE_PACKAGES}/run_coverage_at_startup.pth

    - name: Download and install extensions
      run: |
        echo ""
        echo "Pyomo download-extensions"
        echo ""
        pyomo download-extensions
        echo ""
        echo "Pyomo build-extensions"
        echo ""
        pyomo build-extensions --parallel 2

    - name: Report pyomo plugin information
      run: |
        echo "$PATH"
        pyomo help --solvers || exit 1
        pyomo help --transformations || exit 1
        pyomo help --writers || exit 1

    - name: Run Pyomo tests
      if: matrix.mpi == 0
      run: |
        CATEGORY=
        for cat in ${{matrix.category}}; do
            CATEGORY+=" --cat $cat"
        done
        test.pyomo -v $CATEGORY pyomo `pwd`/pyomo-model-libraries

    - name: Run Pyomo MPI tests
      if: matrix.mpi != 0
      run: |
        # Manually invoke the DAT parser so that parse_table_datacmds.py
        # is fully generated by a single process before invoking MPI
        $PYTHON_EXE -c "from pyomo.dataportal.parse_datacmds import \
            parse_data_commands; parse_data_commands(data='')"
        mpirun -np ${{matrix.mpi}} --oversubscribe $NOSETESTS -v \
            --with-xunit --xunit-file=TEST-pyomo-mpi.xml \
            --eval-attr="mpi and (not fragile)" \
            pyomo `pwd`/pyomo-model-libraries

    - name: Run documentation tests
      if: matrix.skip_doctest == 0
      run: |
        make -C doc/OnlineDocs doctest -d

    - name: Process code coverage report
      run: |
        coverage combine
        coverage report -i
        coverage xml -i

    - name: Record build artifacts
      uses: actions/upload-artifact@v2
      with:
        name: ${{github.job}}_${{env.GHA_JOBGROUP}}-${{env.GHA_JOBNAME}}
        path: |
          .coverage
          coverage.xml
          # In general, do not record test results as artifacts to
          #   manage total artifact storage
          # TEST-*.xml

  cover:
    name: process-coverage-${{ matrix.TARGET }}
    needs: build
    if: always() # run even if a build job fails
    runs-on: ${{ matrix.os }}
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-18.04, macos-latest, windows-latest]

        include:
        - os: ubuntu-18.04
          TARGET: linux
        - os: macos-latest
          TARGET: osx
        - os: windows-latest
          TARGET: win

    steps:
    - name: Checkout Pyomo source
      uses: actions/checkout@v2
      # We need the source for .codecov.yml and running "coverage xml"

    - name: Pip package cache
      uses: actions/cache@v2
      id: pip-cache
      with:
        path: cache/pip
        key: pip-${{env.CACHE_VER}}.0-${{runner.os}}-3.8

    - name: Download build artifacts
      uses: actions/download-artifact@v2
      with:
        path: artifacts

    - name: Set up Python 3.8
      uses: actions/setup-python@v2
      with:
        python-version: 3.8

    - name: Install Python Packages (pip)
      shell: bash # DO NOT REMOVE: see note above
      run: |
        python -c 'import sys;print(sys.executable)'
        python -m pip install --cache-dir cache/pip --upgrade pip
        pip install --cache-dir cache/pip ${PYTHON_CORE_PKGS} \
            ${PYTHON_REQUIRED_PKGS}
        python -c 'import sys; print("PYTHON_EXE=%s" \
            % (sys.executable,))' >> $GITHUB_ENV

    - name: Install Pyomo and PyUtilib
      run: |
        echo ""
        echo "Clone Pyomo-model-libraries..."
        git clone https://github.com/Pyomo/pyomo-model-libraries.git
        echo ""
        echo "Install PyUtilib..."
        echo ""
        $PYTHON_EXE -m pip install git+https://github.com/PyUtilib/pyutilib
        echo ""
        echo "Install Pyomo..."
        echo ""
        $PYTHON_EXE setup.py develop ${{matrix.setup_options}}
        echo ""
        echo "Set custom PYOMO_CONFIG_DIR"
        echo ""
        echo "PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config" >> $GITHUB_ENV

    - name: Generate parse_table_datacmds
      run: |
        # Manually invoke the DAT parser so that parse_table_datacmds.py
        # is generated before running "coverage xml"
        $PYTHON_EXE -c "from pyomo.dataportal.parse_datacmds import \
            parse_data_commands; parse_data_commands(data='')"

    - name: Update codecov uploader
      run: |
        set +e
        CODECOV="${GITHUB_WORKSPACE}/codecov.sh"
        echo "CODECOV=$CODECOV" >> $GITHUB_ENV
        for i in `seq 3`; do
            echo "Downloading current codecov script (attempt ${i})"
            curl -L https://codecov.io/bash -o $CODECOV
            if test $? == 0; then
                break
            fi
            DELAY=$(( RANDOM % 30 + 30))
            echo "Pausing $DELAY seconds before re-attempting download"
            sleep $DELAY
        done
        if test ! -e $CODECOV; then
            echo "Failed to download codecov.sh"
            exit 1
        fi

    - name: Upload codecov reports
      run: |
        set +e
        if [ "$GITHUB_EVENT_NAME" == "pull_request" ]; then
            SHA=$(jq --raw-output .pull_request.head.sha "$GITHUB_EVENT_PATH")
        else
            SHA=$GITHUB_SHA
        fi
        function upload {
            echo ""
            echo "Build group: $1"
            export CODECOV_NAME=$1
            export TAG=$1
            export GHA_OS_NAME=${{matrix.TARGET}}
            shift
            rm -vf .coverage coverage.xml
            echo "    $@" | sed 's/ /\n    /'
            coverage combine --debug=dataio $@
            if test ! -e .coverage; then
                echo "No coverage to upload."
                return
            fi
            coverage xml || coverage xml -i
            i=0
            while : ; do
                ((i+=1))
                rm -f codecov.log
                echo "Uploading coverage to codecov (attempt ${i})"
                bash $CODECOV -Z -e TAG,GHA_OS_NAME -X gcov -X s3 \
                    -f coverage.xml -C $SHA | tee codecov.log
                if test $? == 0; then
                    echo "PASS $CODECOV_NAME" >> codecov.result
                    break
                elif test $i -ge 2; then
                    if test `grep successfully codecov.log | wc -l` -gt 0; then
                        echo "PASS $CODECOV_NAME (implied)" >> codecov.result
                    else
                        # Do not fail the build (yet) just because the
                        # codecov upload fails
                        echo "FAIL $CODECOV_NAME" >> codecov.result
                        break
                    fi
                fi
                DELAY=$(( RANDOM % 30 + 30))
                echo "Pausing $DELAY seconds before re-attempting upload"
                sleep $DELAY
            done
        }
        for ARTIFACT in artifacts/*_*${{matrix.TARGET}}_*; do
            NAME=`echo $ARTIFACT | cut -d/ -f2`
            cp -v $ARTIFACT/.coverage .coverage-$NAME
        done
        upload ${{matrix.TARGET}} .coverage-*_${{matrix.TARGET}}-*
        if compgen -G ".coverage-*" > /dev/null; then
            upload ${{matrix.TARGET}}/other .coverage-*
        fi
        # Legacy builds that don't support coverage 5.x
        if compgen -G ".coverage-*" > /dev/null; then
            echo ""
            echo "Installing coverage 4.x for legacy builds"
            pip install 'coverage<5'
            upload ${{matrix.TARGET}}/legacy .coverage-*
        fi

    - name: Check codecov upload success
      run: |
        FAIL=`grep FAIL codecov.result | wc -l`
        ALL=`cat codecov.result | wc -l`
        echo "$FAIL of $ALL codecov uploads failed"
        if test $FAIL -gt 0; then
            grep FAIL codecov.result | sed 's/^/    /'
            if test $FAIL -gt 1; then
                echo "More than 1 codecov upload failed."
                exit 1
            fi
        fi
