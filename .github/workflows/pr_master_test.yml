name: GitHub CI

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
  workflow_dispatch:
    inputs:
      git-ref:
        description: Git Hash (Optional)
        required: false

defaults:
  run:
    shell: bash -l {0}

env:
  PYTHONWARNINGS: ignore::UserWarning
  PYTHON_REQUIRED_PKGS: >
      ply six nose coverage
  PYTHON_BASE_PKGS: >
      cython dill ipython networkx openpyxl pathos
      pint pymysql pyro4 pyyaml sphinx_rtd_theme sympy xlrd wheel
      python-louvain
  PYTHON_NUMPY_PKGS: >
      numpy scipy pyodbc pandas matplotlib seaborn
  CACHE_VER: v2

jobs:
  build:
    name: ${{ matrix.TARGET }}/${{ matrix.python }}${{ matrix.NAME }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python: [2.7, 3.5, 3.6, 3.7, 3.8, pypy2, pypy3]
        mpi: [0]
        include:
        - os: ubuntu-latest
          TARGET: linux
          PYENV: pip

        - os: macos-latest
          TARGET: osx
          PYENV: pip

        - os: windows-latest
          TARGET: win
          PYENV: conda
          PACKAGES: glpk

        - os: ubuntu-latest
          python: 3.7
          mpi: 3
          skip_doctest: 1
          TARGET: linux
          PYENV: conda
          PACKAGES: mpi4py openmpi
          NAME: /mpi

        exclude:
        - {os: macos-latest, python: pypy2}
        - {os: macos-latest, python: pypy3}
        - {os: windows-latest, python: pypy2}
        - {os: windows-latest, python: pypy3}

    steps:
    - uses: actions/checkout@v2

    - name: Configure job parameters
      run: |
        JOB="${{matrix.TARGET}}/${{matrix.python}}${{matrix.NAME}}"
        echo "GHA_JOBNAME=$JOB" | sed 's|/|_|g' >> $GITHUB_ENV

    # Ideally we would cache the conda downloads; however, each cache is
    # over 850MB, and with 5 python versions, that would consume 4.2 of
    # the 5 GB GitHub allows.
    #
    #- name: Conda package cache
    #  uses: actions/cache@v1
    #  if: matrix.PYENV == 'conda'
    #  id: conda-cache
    #  with:
    #    path: cache/conda
    #    key: conda-${{env.CACHE_VER}}.0-${{runner.os}}-${{matrix.python}}

    - name: Pip package cache
      uses: actions/cache@v1
      if: matrix.PYENV == 'pip'
      id: pip-cache
      with:
        path: cache/pip
        key: pip-${{env.CACHE_VER}}.0-${{runner.os}}-${{matrix.python}}

    - name: OS package cache
      uses: actions/cache@v1
      if: matrix.TARGET != 'osx'
      id: os-cache
      with:
        path: cache/os
        key: pkg-${{env.CACHE_VER}}.0-${{runner.os}}

    - name: TPL package download cache
      uses: actions/cache@v1
      id: download-cache
      with:
        path: cache/download
        key: download-${{env.CACHE_VER}}.0-${{runner.os}}

    - name: Configure curl
      run: |
        CURLRC="$(cat <<EOF
           retry = 0
           max-time = 30
        EOF
        )"
        echo "$CURLRC" > ${GITHUB_WORKSPACE}/.curlrc
        echo "$CURLRC" > ${GITHUB_WORKSPACE}/_curlrc
        echo "CURL_HOME=$GITHUB_WORKSPACE" >> $GITHUB_ENV

    - name: Update OSX
      if: matrix.TARGET == 'osx'
      run: |
        mkdir -p ${GITHUB_WORKSPACE}/cache/os
        export HOMEBREW_CACHE=${GITHUB_WORKSPACE}/cache/os
        # Be cautious running brew update: it can break
        #    setup-python on OSX
        # brew update
        #
        # Notes:
        #  - install glpk
        #  - pyodbc needs: gcc pkg-config unixodbc freetds
        for pkg in bash pkg-config unixodbc freetds glpk; do
            brew list $pkg || brew install $pkg
        done
        #brew link --overwrite gcc

    - name: Update Linux
      if: matrix.TARGET == 'linux'
      run: |
        mkdir -p ${GITHUB_WORKSPACE}/cache/os
        # Notes:
        #  - install glpk
        #  - ipopt needs: libopenblas-dev gfortran liblapack-dev
        sudo apt-get -o Dir::Cache=${GITHUB_WORKSPACE}/cache/os \
            install libopenblas-dev gfortran liblapack-dev glpk-utils
        sudo chmod -R 777 ${GITHUB_WORKSPACE}/cache/os

    - name: Set up Python ${{ matrix.python }}
      if: matrix.PYENV == 'pip'
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python }}

    - name: Set up Miniconda Python ${{ matrix.python }}
      if: matrix.PYENV == 'conda'
      uses: conda-incubator/setup-miniconda@v1
      with:
        auto-update-conda: true
        python-version: ${{ matrix.python }}

    # GitHub actions is very fragile when it comes to setting up various
    # Python interpreters, expecially the setup-miniconda interface.
    # Per the setup-miniconda documentation, it is important to always
    # invoke bash as a login shell ('shell: bash -l {0}') so that the
    # conda environment is properly activated.  However, running within
    # a login shell appears to foul up the link to python from
    # setup-python.  Further, we have anecdotal evidence that
    # subprocesses invoked through $(python -c ...) and `python -c ...`
    # will not pick up the python activated by setup-python on OSX.
    #
    # Our solution is to define a PYTHON_EXE environment variable that
    # can be explicitly called within subprocess calls to reach the
    # correct interpreter.  Note that we must explicitly run in a *non*
    # login shell to set up the environment variable for the
    # setup-python environments.

    - name: Install Python Packages (pip)
      if: matrix.PYENV == 'pip'
      shell: bash # DO NOT REMOVE: see note above
      run: |
        python -c 'import sys;print(sys.executable)'
        python -m pip install --cache-dir cache/pip --upgrade pip
        # Note: pandas 1.0.3 causes gams 29.1.0 import to fail in python 3.8
        pip install --cache-dir cache/pip ${PYTHON_BASE_PKGS} \
            ${PYTHON_REQUIRED_PKGS} ${{matrix.PACKAGES}}
        if [[ ${{matrix.python}} != pypy* ]]; then
            # NumPy and derivatives either don't build under pypy, or if
            # they do, the builds take forever.
            pip install --cache-dir cache/pip ${PYTHON_NUMPY_PKGS}
        fi
        pip install --cache-dir cache/pip cplex \
            || echo "WARNING: CPLEX Community Edition is not available"
        pip install --cache-dir cache/pip xpress \
            || echo "WARNING: Xpress Community Edition is not available"
        python -c 'import sys; print("PYTHON_EXE=%s" \
            % (sys.executable,))' >> $GITHUB_ENV
        echo "NOSETESTS="`which nosetests` >> $GITHUB_ENV

    - name: Install Python packages (conda)
      if: matrix.PYENV == 'conda'
      run: |
        mkdir -p $GITHUB_WORKSPACE/cache/conda
        conda config --set always_yes yes
        conda config --set auto_update_conda false
        conda config --prepend pkgs_dirs $GITHUB_WORKSPACE/cache/conda
        conda info
        conda config --show-sources
        conda list --show-channel-urls
        conda install -q -y -c conda-forge ${PYTHON_BASE_PKGS} \
            ${PYTHON_NUMPY_PKGS} ${PYTHON_REQUIRED_PKGS} ${{matrix.PACKAGES}}
        # Note: CPLEX 12.9 (the last version in conda that supports
        #    Python 2.7) causes a seg fault in the tests.
        conda install -q -y -c ibmdecisionoptimization cplex=12.10 \
            || echo "WARNING: CPLEX Community Edition is not available"
        conda install -q -y -c fico-xpress xpress \
            || echo "WARNING: Xpress Community Edition is not available"
        python -c 'import sys; print("PYTHON_EXE=%s" \
            % (sys.executable,))' >> $GITHUB_ENV
        echo "NOSETESTS="`which nosetests` >> $GITHUB_ENV

    - name: Setup TPL package directories
      run: |
        TPL_DIR="${GITHUB_WORKSPACE}/cache/tpl"
        mkdir -p "$TPL_DIR"
        DOWNLOAD_DIR="${GITHUB_WORKSPACE}/cache/download"
        mkdir -p "$DOWNLOAD_DIR"
        echo "TPL_DIR=$TPL_DIR" >> $GITHUB_ENV
        echo "DOWNLOAD_DIR=$DOWNLOAD_DIR" >> $GITHUB_ENV

    - name: Install Ipopt
      run: |
        IPOPT_DIR=$TPL_DIR/ipopt
        echo "$IPOPT_DIR" >> $GITHUB_PATH
        mkdir -p $IPOPT_DIR
        IPOPT_TAR=${DOWNLOAD_DIR}/ipopt.tar.gz
        if test ! -e $IPOPT_TAR; then
            echo "...downloading Ipopt"
            URL=https://github.com/IDAES/idaes-ext/releases/download/2.0.0
            if test "${{matrix.TARGET}}" == osx; then
                echo "IDAES Ipopt not available on OSX"
                exit 0
            elif test "${{matrix.TARGET}}" == linux; then
                curl --max-time 150 --retry 8 \
                    -L $URL/idaes-solvers-ubuntu1804-64.tar.gz \
                    > $IPOPT_TAR
            else
                curl --max-time 150 --retry 8 \
                    -L $URL/idaes-solvers-windows-64.tar.gz \
                    $URL/idaes-lib-windows-64.tar.gz > $IPOPT_TAR
            fi
        fi
        cd $IPOPT_DIR
        tar -xzi < $IPOPT_TAR

    - name: Install GAMS
      # We install using Powershell because the GAMS installer hangs
      # when launched from bash on Windows
      shell: pwsh
      run: |
        $GAMS_DIR="${env:TPL_DIR}/gams"
        echo "$GAMS_DIR" >> $GITHUB_PATH
        echo "LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$GAMS_DIR" >> $GITHUB_ENV
        echo "DYLD_LIBRARY_PATH=${DYLD_LIBRARY_PATH}:$GAMS_DIR" >> $GITHUB_ENV
        $INSTALLER="${env:DOWNLOAD_DIR}/gams_install.exe"
        $URL="https://d37drm4t2jghv5.cloudfront.net/distributions/29.1.0"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            $URL = "$URL/windows/windows_x64_64.exe"
        } elseif ( "${{matrix.TARGET}}" -eq "osx" ) {
            $URL = "$URL/macosx/osx_x64_64_sfx.exe"
        } else {
            $URL = "$URL/linux/linux_x64_64_sfx.exe"
        }
        if (-not (Test-Path "$INSTALLER" -PathType Leaf)) {
            echo "...downloading GAMS"
            Invoke-WebRequest -Uri "$URL" -OutFile "$INSTALLER"
        }
        echo "...installing GAMS"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            Start-Process -FilePath "$INSTALLER" -ArgumentList `
                "/SP- /NORESTART /VERYSILENT /DIR=$GAMS_DIR /NOICONS" `
                -Wait
        } else {
            chmod 777 $INSTALLER
            Start-Process -FilePath "$INSTALLER" -ArgumentList `
                "-q -d $GAMS_DIR" -Wait
            mv $GAMS_DIR/*/* $GAMS_DIR/.
        }

    - name: Install GAMS Python bindings
      run: |
        GAMS_DIR="${env:TPL_DIR}/gams"
        py_ver=$($PYTHON_EXE -c 'import sys;v="_%s%s" % sys.version_info[:2] \
            ;print(v if v != "_27" else "")')
        if test -e $GAMS_DIR/apifiles/Python/api$py_ver; then
            echo "Installing GAMS Python bindings"
            pushd $GAMS_DIR/apifiles/Python/api$py_ver
            $PYTHON_EXE setup.py install
            popd
        fi

    - name: Install BARON
      shell: pwsh
      run: |
        $BARON_DIR="${env:TPL_DIR}/baron"
        echo "$BARON_DIR" >> $GITHUB_PATH
        $URL="https://www.minlp.com/downloads/xecs/baron/current/"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            $INSTALLER = "${env:DOWNLOAD_DIR}/baron_install.exe"
            $URL += "baron-win64.exe"
        } elseif ( "${{matrix.TARGET}}" -eq "osx" ) {
            $INSTALLER = "${env:DOWNLOAD_DIR}/baron_install.zip"
            $URL += "baron-osx64.zip"
        } else {
            $INSTALLER = "${env:DOWNLOAD_DIR}/baron_install.zip"
            $URL += "baron-lin64.zip"
        }
        if (-not (Test-Path "$INSTALLER" -PathType Leaf)) {
            echo "...downloading BARON ($URL)"
            Invoke-WebRequest -Uri "$URL" -OutFile "$INSTALLER"
        }
        echo "...installing BARON"
        if ( "${{matrix.TARGET}}" -eq "win" ) {
            Start-Process -FilePath "$INSTALLER" -ArgumentList `
                "/SP- /NORESTART /VERYSILENT /DIR=$BARON_DIR /NOICONS" `
                -Wait
        } else {
            unzip -q $INSTALLER
            mv baron-* $BARON_DIR
        }

    - name: Install GJH_ASL_JSON
      if: matrix.TARGET != 'win'
      run: |
        GJH_DIR="$TPL_DIR/gjh"
        echo "${GJH_DIR}" >> $GITHUB_PATH
        INSTALL_DIR="${DOWNLOAD_DIR}/gjh"
        if test ! -e "$INSTALL_DIR/bin"; then
            mkdir -p "$INSTALL_DIR"
            INSTALLER="$INSTALL_DIR/gjh_asl_json.zip"
            URL="https://codeload.github.com/ghackebeil/gjh_asl_json/zip/master"
            curl --max-time 150 --retry 8 -L $URL > $INSTALLER
            cd $INSTALL_DIR
            unzip -q $INSTALLER
            cd gjh_asl_json-master/Thirdparty
            ./get.ASL
            cd ..
            make
            mv bin "$INSTALL_DIR/bin"
        fi
        cp -rp "$INSTALL_DIR/bin" "$GJH_DIR"

    - name: Install Pyomo and PyUtilib
      run: |
        echo ""
        echo "Clone Pyomo-model-libraries..."
        git clone https://github.com/Pyomo/pyomo-model-libraries.git
        echo ""
        echo "Install PyUtilib..."
        echo ""
        $PYTHON_EXE -m pip install git+https://github.com/PyUtilib/pyutilib
        echo ""
        echo "Install Pyomo..."
        echo ""
        $PYTHON_EXE setup.py develop
        echo ""
        echo "Set custom PYOMO_CONFIG_DIR"
        echo ""
        echo "PYOMO_CONFIG_DIR=${GITHUB_WORKSPACE}/config" >> $GITHUB_ENV

    - name: Set up coverage tracking
      run: |
        if test "${{matrix.TARGET}}" == win; then
            COVERAGE_BASE=${GITHUB_WORKSPACE}\\.cover
        else
            COVERAGE_BASE=${GITHUB_WORKSPACE}/.cover
        fi
        COVERAGE_RC=${COVERAGE_BASE}_rc
        echo "COVERAGE_RCFILE=$COVERAGE_RC" >> $GITHUB_ENV
        echo "COVERAGE_PROCESS_START=$COVERAGE_RC" >> $GITHUB_ENV
        cp ${GITHUB_WORKSPACE}/.coveragerc ${COVERAGE_RC}
        echo "data_file=${COVERAGE_BASE}age" >> ${COVERAGE_RC}
        SITE_PACKAGES=$($PYTHON_EXE -c "from distutils.sysconfig import \
            get_python_lib; print(get_python_lib())")
        echo "Python site-packages: $SITE_PACKAGES"
        echo 'import coverage; coverage.process_startup()' \
            > ${SITE_PACKAGES}/run_coverage_at_startup.pth

    - name: Download and install extensions
      run: |
        echo ""
        echo "Pyomo download-extensions"
        echo ""
        pyomo download-extensions
        echo ""
        echo "Pyomo build-extensions"
        echo ""
        pyomo build-extensions --parallel 2

    - name: Report pyomo plugin information
      run: |
        pyomo help --solvers || exit 1
        pyomo help --transformations || exit 1
        pyomo help --writers || exit 1

    - name: Run Pyomo tests
      if: matrix.mpi == 0
      run: |
        test.pyomo -v --cat="nightly" pyomo `pwd`/pyomo-model-libraries

    - name: Run Pyomo MPI tests
      if: matrix.mpi != 0
      run: |
        # Manually invoke the DAT parser so that parse_table_datacmds.py
        # is fully generated by a single process before invoking MPI
        $PYTHON_EXE -c "from pyomo.dataportal.parse_datacmds import \
            parse_data_commands; parse_data_commands(data='')"
        mpirun -np ${{matrix.mpi}} --oversubscribe $NOSETESTS -v \
            --with-xunit --xunit-file=TEST-pyomo-mpi.xml \
            --eval-attr="mpi and (not fragile)" \
            pyomo `pwd`/pyomo-model-libraries

    - name: Run documentation tests
      if: matrix.skip_doctest == 0
      run: |
        make -C doc/OnlineDocs doctest -d

    - name: Process code coverage report
      env:
        CODECOV_NAME: ${{matrix.TARGET}}/${{matrix.python}}${{matrix.NAME}}
      run: |
        coverage combine
        coverage report -i
        coverage xml -i
        set +e
        # Always attempt to update the codecov script, but fall back on
        # the previously cached script if it fails
        CODECOV="${GITHUB_WORKSPACE}/cache/download/codecov.sh"
        for i in `seq 3`; do
            echo "Downloading current codecov script (attempt ${i})"
            curl -L https://codecov.io/bash -o $CODECOV
            if test $? == 0; then
                break
            fi
            DELAY=$(( RANDOM % 30 + 30))
            echo "Pausing $DELAY seconds before re-attempting download"
            sleep $DELAY
        done
        i=0
        while : ; do
            ((i+=1))
            echo "Uploading coverage to codecov (attempt ${i})"
            bash $CODECOV -Z -X gcov -X s3 -f coverage.xml
            if test $? == 0; then
                echo "PASS" > ${GITHUB_WORKSPACE}/codecov.result
                break
            elif test $i -ge 2; then
                # Do not fail the build just because the codecov upload fails
                echo "FAIL" > ${GITHUB_WORKSPACE}/codecov.result
                break
            fi
            DELAY=$(( RANDOM % 30 + 30))
            echo "Pausing $DELAY seconds before re-attempting upload"
            sleep $DELAY
        done

    - name: Record build artifacts
      uses: actions/upload-artifact@v2
      with:
        name: ${{github.job}}_${{env.GHA_JOBNAME}}
        path: |
          codecov.result
        # In general, do not record test results as artifacts to
        #   manage total artifact storage
        # TEST-*.xml

  post:
    name: post-build
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v2
        with:
          path: artifacts

      - name: Check codecov upload success
        run: |
          FAIL=`grep FAIL artifacts/*/codecov.result | wc -l`
          ALL=`ls -1 artifacts/*/codecov.result | wc -l`
          # Fail is more than 1/9 codecov uploads fail
          echo "$FAIL of $ALL codecov uploads failed"
          if test $FAIL -gt 0; then
              grep FAIL artifacts/*/codecov.result | sed 's/^/    /'
              if test $(( $FAIL * 9 )) -gt $ALL; then
                  echo "More than 1/9 codecov uploads failed:"
                  exit 1
              fi
          fi
